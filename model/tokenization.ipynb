{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A string is an immutable sequence of unicode code points\n",
    "# unicode: a standardized, known and documented character\n",
    "# unicode code point can be accessed using ord() on a single character\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode is standard but it is not stable because it is constantly being updated\n",
    "# instead we use encodings\n",
    "# we can take unicode text and translate it into binary data (most common is utf-8)\n",
    "# utf-8 takes every single code point and translates it into a byte stream that is between 1 and 4 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "----\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode('utf-8') # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers\n",
    "print(text)\n",
    "print('----')\n",
    "print(tokens)\n",
    "print('----')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: grabs all consecutive pairs and counts the occurrences\n",
    "def get_stats(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "#print(sorted(((v, k) for k, v in stats.items()), reverse = True))\n",
    "#chr(101), chr(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 continued: Obtain the highest ranking pair\n",
    "top_pair = max(stats, key = stats.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 2-4: Iteratively replace highest ranking pairs\n",
    "# ids = individual byte encoding for each character\n",
    "# pair = pair we want to replace\n",
    "# idx = what we replace the pair with\n",
    "def sub(tokens, pair, replacement):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # checks if the encodings at the current position matches with the pair\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "            newids.append(replacement)\n",
    "            # skips over the pair\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(tokens[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "#print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 256, 118, 101, 114, 121, 32, 110, 97, 109, 256, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 256, 105, 110, 116, 111, 32, 116, 104, 256, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 256, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 256, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 256, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 256, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 256, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 256, 99, 97, 110, 32, 98, 256, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 256, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 256, 85, 110, 105, 99, 111, 100, 256, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 256, 109, 111, 114, 256, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 256, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 256, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 256, 119, 104, 111, 108, 256, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n"
     ]
    }
   ],
   "source": [
    "print(sub(tokens, top_pair, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (101, 32) into a new token 256\n",
      "merging (240, 159) into a new token 257\n",
      "merging (226, 128) into a new token 258\n",
      "merging (105, 110) into a new token 259\n",
      "merging (115, 32) into a new token 260\n",
      "merging (97, 110) into a new token 261\n",
      "merging (116, 104) into a new token 262\n",
      "merging (257, 133) into a new token 263\n",
      "merging (257, 135) into a new token 264\n",
      "merging (97, 114) into a new token 265\n",
      "merging (239, 189) into a new token 266\n",
      "merging (258, 140) into a new token 267\n",
      "merging (267, 264) into a new token 268\n",
      "merging (101, 114) into a new token 269\n",
      "merging (111, 114) into a new token 270\n",
      "merging (116, 32) into a new token 271\n",
      "merging (259, 103) into a new token 272\n",
      "merging (115, 116) into a new token 273\n",
      "merging (261, 100) into a new token 274\n",
      "merging (32, 262) into a new token 275\n",
      "compression ratio: 1.37X\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m     subs[pair] \u001b[39m=\u001b[39m replacement\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcompression ratio: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(tokens)\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39mlen\u001b[39m(tokens_copy)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39mX\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m \u001b[39mprint\u001b[39m(subs\u001b[39m.\u001b[39;49msplit())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Step 1: grabs all consecutive pairs and counts the occurrences\n",
    "def get_stats(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# Steps 2-4: Iteratively replace highest ranking pairs\n",
    "# tokens = individual byte encoding for each character\n",
    "# pair = pair we want to replace\n",
    "# replacement = the token id of what we replace the pair with\n",
    "def sub(tokens, pair, replacement):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # checks if the encodings at the current position matches with the pair\n",
    "        if i < len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:\n",
    "            newids.append(replacement)\n",
    "            # skips over the pair\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(tokens[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# vocab size: the final length of the vocabulary size we want are tokenizer to have\n",
    "vocab_size = 276\n",
    "# we will do 20 replacements/substituions\n",
    "num_subs = vocab_size - 256\n",
    "# creates a copy of the list\n",
    "tokens_copy = list(tokens)\n",
    "\n",
    "# maintains the mapping of the original to the new token (i.e. keeps track of aa = Z)\n",
    "# subs is equivalent to GPT-2 'vocab.bpe'\n",
    "subs = {}\n",
    "for i in range(num_subs):\n",
    "    stats = get_stats(tokens_copy)\n",
    "    pair = max(stats, key = stats.get)\n",
    "    replacement = 256 + i\n",
    "    print(f'merging {pair} into a new token {replacement}')\n",
    "    tokens_copy = sub(tokens_copy, pair, replacement)\n",
    "    subs[pair] = replacement\n",
    "print(f'compression ratio: {len(tokens) / len(tokens_copy):.2f}X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Given a sequence of integers in the range [0, vocab_size] what is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 32\n",
      "240 159\n",
      "226 128\n",
      "105 110\n",
      "115 32\n",
      "97 110\n",
      "116 104\n",
      "257 133\n",
      "257 135\n",
      "97 114\n",
      "239 189\n",
      "258 140\n",
      "267 264\n",
      "101 114\n",
      "111 114\n",
      "116 32\n",
      "259 103\n",
      "115 116\n",
      "261 100\n",
      "32 262\n"
     ]
    }
   ],
   "source": [
    "# vocab is a mapping/dictionary between the token id and the bytes object of the token\n",
    "# vocab is equivalent to gpt-2 file 'encoder.json'\n",
    "vocab = {replacement: bytes([replacement]) for replacement in range(256)}\n",
    "# we go in order of the substitutions\n",
    "for (p0, p1), replacement in subs.items():\n",
    "    # we add two bytes objects together\n",
    "    vocab[replacement] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(tokens):\n",
    "    # iterate over ids to get byte objects\n",
    "    string = b\"\".join(vocab[replacement] for replacement in tokens)\n",
    "    # not all byte sequences are valid utf-8 (i.e. 128), so we need to include errors = 'replace'\n",
    "    text = string.decode('utf-8', errors = 'replace')\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "Given a string, what are the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    # gives us the raw bytes\n",
    "    tokens = list(text.encode('utf-8'))\n",
    "    # we will do a few merges\n",
    "    while True:\n",
    "        # counts up how often pairs occur in our sequence\n",
    "        stats = get_stats(tokens)\n",
    "        # we want to merge in order which we do by finding the minimum index in dictionary subs\n",
    "        pair = min(stats, key = lambda p: subs.get(p, float('inf')))\n",
    "        # nothing else to substitute\n",
    "        if pair not in subs:\n",
    "            break\n",
    "        replacement = subs[pair]\n",
    "        tokens = sub(tokens, pair, replacement)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 32)\n",
      "(115, 32)\n",
      "(116, 104)\n",
      "(101, 114)\n",
      "(32, 262)\n",
      "(84, 104)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check results\n",
    "text = \"The neon lights flickered as the city whispered its secrets to the night.\"\n",
    "text2 = decode(encode(text))\n",
    "print(text == text2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forced splits using regex patterns (GPT series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence is split up into tokens, each token will be sent through the tokenizer, then the results from the tokenizer will be concatenated together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '56', ' how', ' are', ' you', ' 123', ' 17']\n"
     ]
    }
   ],
   "source": [
    "# Tiktoken 'gpt2'\n",
    "### This is extremely flawed for a few reasons:\n",
    "### 1. only considers lowercase 've, etc. (does not consider 'VE)\n",
    "### 2. only cosiders regular apostrophe ' (does not consider unicode apostrophe Ê¼)\n",
    "### 3. the apostrophe cases are biased towards English\n",
    "# sentence split up into a list of tokens\n",
    "import regex as re\n",
    "#  ?\\p{L}+: optional space followed by letters in any language\n",
    "#  ?\\p{N}+: optional space followed by numbers\n",
    "# \\s+(?!\\S): spaces up to but not including the last character\n",
    "# \\s+: catches any trailing spaces that weren't caught\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "print(re.findall(gpt2pat, 'Hello world56 how are you 123 17'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', '56', ' how', ' are', ' you', ' ', '123', ' ', '17']\n"
     ]
    }
   ],
   "source": [
    "# Tiktoken 'cl100k_base' (tokenizer for GPT-4)\n",
    "import regex as re\n",
    "# i: case insensitive match\n",
    "# [sdmt] same as 'gpt2', just makes it faster\n",
    "# \\p{N}{1,3}: only merges up to three digits to prevent tokens made of a long sequence of numbers\n",
    "gpt2pat = re.compile(r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "print(re.findall(gpt2pat, 'Hello world56 how are you 123 17'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 995, 3980, 703, 389, 345, 17031, 1596] Hello world56 how are you 123 17\n",
      "[9906, 1917, 3487, 1268, 527, 499, 220, 4513, 220, 1114] Hello world56 how are you 123 17\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# tiktoken also includes a byte decode and encode portion\n",
    "# byte encode -> encode -> transformer/LLM work -> decode -> byte decode\n",
    "# otherwise, their encoder.py (bpe function) is very similar to our work above\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "ins = enc.encode('Hello world56 how are you 123 17')\n",
    "print(ins, enc.decode(ins))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding('cl100k_base')\n",
    "ins = enc.encode('Hello world56 how are you 123 17')\n",
    "print(ins, enc.decode(ins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-06 15:05:57--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.179.33\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.179.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: â€˜vocab.bpeâ€™\n",
      "\n",
      "vocab.bpe           100%[===================>] 445.62K  1.68MB/s    in 0.3s    \n",
      "\n",
      "2024-05-06 15:05:58 (1.68 MB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
      "\n",
      "--2024-05-06 15:05:58--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 20.60.179.33\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|20.60.179.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: â€˜encoder.jsonâ€™\n",
      "\n",
      "encoder.json        100%[===================>]   1018K  2.15MB/s    in 0.5s    \n",
      "\n",
      "2024-05-06 15:05:59 (2.15 MB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the OpenAI vocab: 50257\n"
     ]
    }
   ],
   "source": [
    "print(f'The length of the OpenAI vocab: {len(encoder)}') # 256 raw byte tokens and OpenAI fif 50,000 merges +1 special token '<|endoftext|>'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>'] # this doesnt go through the bpe merges\n",
    "# instead, there is a special set of instructions for how to handle this token (tiktoken uses a Rust .rs file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentencepiece\n",
    "can do both training and inference\n",
    "\n",
    "works directly at the code points themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace1, replace2 = map(int, ['1', '15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('3.9.15')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2aedbfd189cae87eb21ab424de367096cc6446e857e09377980d0d873056986"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
